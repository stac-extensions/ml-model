services:
  model-inference:
    image: docker.io/someusername/some_model_image:1
    volumes:
      - "${INPUT_VOLUME}:/var/data/input"
      - "${OUTPUT_VOLUME}:/var/data/output"
    entrypoint: bash /app/scripts/run-model.sh

